Setting up MLOps Pipeline in AWS:

1. Create Data Repositories      

- Creating S3 Buckets: Create one S3 bucket to store raw and pre-processed machine learning training data and another to store output of feature engineering tasks (acts as a source repository for the model training stage).          
- AWS Code Commit: Repository in AWS CodeCommit to store the machine learning source code, pipeline configuration files, and other related code assets and clone this repository into the AWS Cloud9 environment for further development and tracking.    
- Elastic Container Registry (ECR): Repository for storing training and inference container images.     
- repository_validation.py: Script to validate the creation and configuration of these resources.     

2. Create Pipeline Assets

- ETL Job File (preprocess.py): Script to handle automation of data pre-processing and feature engineering. It includes a function to split the dataset into training, validation, and test sets and performs tasks such as one-hot encoding and data restructuring. The processed data is then uploaded to an S3 bucket.    

- ETL Job Configuration (job.json): This configuration file specifies that the job is a Python shell job, setting the Python version and the job's purpose (featurizing the dataset).   

- Model Training file (model.py in model folder): Contains the training function for a deep learning neural network for regression tasks, using TensorFlow. 

- API  and Web Server Files (app.py, wsgi.py, Nginx configuration): The app.py script sets up a Flask application to provide a prediction service API.
The wsgi.py file acts as a web server gateway interface.
Nginx configuration is included for setting up the web server.      

- Docker File: Specifies the instructions to assemble the container image, including setting up the working directory, copying necessary files, and exposing the required port.    

- Build Configuration (build.py): A Python script that generates CloudFormation parameters unique to both development and production environments. It selects the latest approved model package and generates the necessary parameters for deployment.   

- CodeBuild Specification (buildspec.yaml): Contains instructions for AWS CodeBuild to configure the CloudFormation parameters generated by build.py.    

- SageMaker Training Job Configuration (trainingjob.json): Specifies the configuration for the Amazon SageMaker training job, including the instance type used for training.  

- System Test Files (in test/system_test folder):
build.py constructs an automated workflow using AWS Step Functions, defining the steps involved in the system test.      
buildspec.yaml contains instructions for building the system test workflow.     

- Assets folder: Cloud formation templates detailing resources like SageMaker models and endpoints, specifying configurations for development and production environments.   

3. Perform unit testing
- Build the docker container     
- Test the training function
- Test the prediction function
- Test the model server API

4. Perform system test

5. Pipeline Components - mlops-pipeline.yml:

- Parameters: Defines customizable parameters like ImageRepoName, ImageTagName, ModelName, and RoleName. These parameters are used throughout the template to create resources with specific names and configurations.     

- Lambda Functions: 
CreateModelGroup: A Lambda function to create a SageMaker Model Package Group for versioned, production-grade models.     
TrainingLaunchJob: Starts a new SageMaker Training Job.     
EtlLaunchJob: Initiates a new AWS Glue ETL Job.    
TrainingJobMonitor and EtlJobMonitor: Functions to monitor the status of the SageMaker Training Job and Glue ETL Job, respectively.     

- Lambda Permissions:            TrainingJobMonitorPermissions and EtlJobMonitorPermissions: Set the necessary permissions for the Lambda functions to be invoked by CloudWatch events.     

- CloudWatch Events: TrainingJobMonitoringEvent and EtlJobMonitoringEvent: These rules define events that monitor the completion of the training and ETL jobs and inform the CodePipeline about their completion.            

- AWS CodeBuild Projects:    
BuildImageProject: Builds the model training and inference container images.     
BuildDeploymentProject: Builds required resource properties for deployment.     
BuildWorkflowProject: Constructs the system test workflow using AWS Step Functions.   

- AWS CodePipeline (MLOpsPipeline):
Describes the stages in the machine learning operations workflow, detailing each stage's actions, such as source retrieval, image building, ETL job execution, model training, approvals, and deployments to development and production environments.    

- IAM Role (MLOpsRole):        
Configures an IAM role with a comprehensive policy that grants necessary permissions across various AWS services involved in the pipeline. This role is assumed by the pipeline and its components to perform required actions.

- Artifacts and Environment Variables:        
Defines artifacts and environment variables for CodeBuild projects, tailoring them to specific tasks like image building, deployment, and workflow creation.       

6. Create the pipeline    
- Upload local lambda resources to S3 Bucket
- upload dataset to the S3 bucket
- Create cloud formation stack





